\section{SIモデル}
漁獲量を単一の海洋環境変数から説明するモデルがSIモデルである．以下では提案手法でのSIモデル構築手法について述べる．
本研究で使用する海洋環境変数データは5種の変数，塩分濃度，温度，緯度方向の流速，経度方向の流速，深さ方向の流速からなる．
また各変数は特定の深さごとに値を持っており，それらの線形和を使ってSIモデル構築をする場合，次式で表すことができる．

\begin{eqnarray}
SI = \sum_{i=0}^{depth} a_i x_i
\label{eq:3-1}
\end{eqnarray}

$x_i$ は漁獲点の鉛直方向深さiでの海洋環境変数の値，$a_i$ はその係数である．
さらに過去のデータとして，先に説明した手法で計算した海洋環境変数データを用いると次式でSIモデルを表せる．

\begin{eqnarray}
SI = \sum_{j=0}^{days} \sum_{i=0}^{depth} a_{ij} x_{ij}
\label{eq:3-5}
\end{eqnarray}

$x_{ij}$ は漁獲点の鉛直方向の深さiを始点としてj日遡った地点での海洋環境変数の値，$a_{ij}$ はその係数である．

漁獲点における式(\ref{eq:3-1})のSI値を漁獲量予測に変換した値と漁獲実績(CPUE)との差の絶対値を最小化する $a_{ij}$ を求めることでSIモデルを構築できる（式\ref{eq:3-6}）．

\begin{eqnarray}
\label{eq:3-6}
& & \argmin_{a_{ij}} (|CPUE-PredictedCPUE|) \\
&=& \argmin_{a_{ij}} \left( \left| CPUE-(maxCPUE*\sum_{j=0}^{days} \sum_{i=0}^{depth} a_{ij} x_{ij} ) \right| \right)
\end{eqnarray}

また，実測した漁獲点は複数存在するのでこれを行列で表すことができる（式\ref{eq:3-7}）．

\begin{eqnarray}
\label{eq:3-7}
\argmin_{\theta} ||{\bf Y}-{\bf X}\theta||_{2}^{2}
\end{eqnarray}

{\bf Y}は$n \times 1$ 行列で，i行目はi番目の漁獲点でのCPUEとなっている．
{\bf X}はk番目の漁獲点で成立する式\ref{eq:3-5}の$x_{ij}$をk行目に横に並べた$n \times m$行列である．
$\theta$は$m \times 1$行列で，{\bf X}における海洋環境変数$x_{ij}$に対応する係数行列である．そのためmは$x_{ij}$の数と同じである．
\if0

各行列を実際に書くのもあり
X = (a_{0,0} a_{0,1} ... a_{0,59} a_{0,60} a{1,0} ... a_{54,60} )など

\fi
式\ref{eq:3-7}ではどの漁獲点でも利用可能なモデルを構築しようとしているので，$x_{ij}$はどの漁獲点でも同じ係数となる．
そのため，$\theta$は$n \times m$行列ではなく，$n \times 1$行列となる．
式\ref{eq:3-7}は機械学習の分野で教師あり学習と呼ばれたり，回帰問題と呼ばれる問題で最小二乗法などの解法によって解くことができる．
式\ref{eq:3-7}を解くと全ての変数に0でない係数が現れる．
これは全ての深さの全ての変数が漁獲量に影響するということを意味するが，例えばある深さで対象生物を漁獲する際に，同じ日の全く別の深さの海洋環境変数がその漁獲量に影響するとは考えにくく，海洋の専門家も否定している．
また，すべての変数を漁獲モデルに使用するとオーバーフィッティングの問題が発生する．
そのため，漁獲モデルに全ての変数を利用するのではなく，一部を削減する必要がある．

\section{オーバーフィッティング}
構築したモデルが，構築に使用したデータに対してはよく適合しているが，未知のデータに対して適合していない状態がオーバーフィッティング（過学習）である．
\if0
オーバーフィッティングの一般的な説明と，今回のデータでオーバーフィッティングの例を示したい
\fi
今回のように計算機による自動計算でモデルを構築する際によく問題となる状態である．
本研究の場合であれば，実測した漁獲点で予測させた場合は実際のCPUEに近い値が出るが，その予測モデルを実際の予測に使用すると実際のCPUEとかけ離れたモデルができてしまう状態がオーバーフィッティングである．
一般にオーバーフィッティングはモデルの自由度が高すぎる場合に発生する．
そこで，式\ref{eq:3-5}のように使用可能な全ての変数を使用するのではなく，変数の数を減らす工夫をする．
以下で説明する変数の数を減らすモデル構築手法はスパースモデリングとも呼ばれる．
係数行列$\theta$の成分に0の項があれば，実質的に変数の数が減っていることになる．
これがスパースモデリングの基本的な変数削減手法であり，その手法にはRidge回帰\cite{a9}，Lasso回帰\cite{a8}など様々な手法がある．
Ridge回帰は古くに提案された変数削減手法であり次のように行う．
式\ref{eq:3-7}で示した最小化式に罰則項としてL2ノルムを加えたものである．
Lpノルムは一般に$p \geq 1$で定義されるノルムで，式\ref{eq:3-8}で表せる．

\begin{eqnarray}
\label{eq:3-8}
Lp norm = ||x||_p = (|x_0|^p + |x_1|^p + |x_2|^p + ...)^{\frac{1}{p}}
\end{eqnarray}

p=2の場合がL2ノルムであり，次式で示すように各係数の二乗和の平方根となる．

\begin{eqnarray}
\label{eq:3-85}
L2norm = ||x||_2 = (|x_0|^2 + |x_1|^2 + |x_2|^2 + ...)^{\frac{1}{2}} = \sqrt{\sum_i |x_i|^2}
\end{eqnarray}

これを罰則項とした上で罰則項そのものの重み$\lambda$も考慮し，式\ref{eq:3-7}に対し適用すると次式となる．

\begin{eqnarray}
\label{eq:3-9}
\argmin_{\theta} \left( ||{\bf Y}-{\bf X}\theta||_{2}^{2} + \lambda \sqrt{\sum_i |\theta_i |^2} \right)
\end{eqnarray}

ただし，$\theta_i$は$\theta$のi番目の成分である．
式\ref{eq:3-9}がRidge回帰である．
式\ref{eq:3-9}は次のようにして解く．

式\ref{eq:3-9}は最小化を行いたいので$\theta$で偏微分すると0になる（式\ref{eq:3-9-1}）．

\begin{eqnarray}
\label{eq:3-9-1}
2 X^T X \theta - 2 X^T Y + 2 \lambda \theta = 0
\end{eqnarray}

これを整理すると，解くべき方程式は式\ref{eq:3-9-2}となる．

\begin{eqnarray}
\label{eq:3-9-2}
(X^T X + \lambda I)\theta = X^T Y
\end{eqnarray}

式\ref{eq:3-9-2}を満たす$\theta$は$X^T X + \lambda I$の逆行列を求めることで求めることができる．
これによってRidge回帰は解くことができる．

このようにして求めた解はオーバーフィッティングの問題が発生しないようなスパースなモデルになっているだろうか．
Ridge回帰では罰則項が係数の二乗和の平方根であり，変数の数（係数の非0の項の数）については一切制約がない．
しかし実際にはモデルに与える影響の小さい変数に対応する係数は0に近づき，閾値処理によって係数0にできる．
これは次の理由で説明できる．
まず，一般化したRidge回帰において，最小化式は式\ref{eq:3-10}となる．

\begin{eqnarray}
\label{eq:3-10}
min \left( E({\bf w}) + \lambda \sqrt{\sum_{i} | w_i |^{2}} \right)
\end{eqnarray}

この目的関数を$w_i$で微分すると次式になる．

\begin{eqnarray}
\label{eq:3-11}
\frac{\partial E({\bf w})}{\partial w_i} +  \lambda w_i
\end{eqnarray}

式\ref{eq:3-11}の第一項は特定の$w_i$で0になり，それ未満では正，それより大きい時は負となる．
第二項は$w_i = 0$で0になり，それ未満では負，それより大きい時は正となる．
また，線形回帰の場合第一項は階段関数となる．
そして第一項はその変数がモデルに与える影響の大小に比例する．
そのため，影響が小さい変数の場合，第二項が係数$w_i$決定に支配的となり$w_i$は0に近づく．
一方，影響の大きい変数の場合，第二項が$w_i$決定にはあまり寄与しなくなり，$w_i$はあまり0に近づかない．
モデルに対する影響の大小によって罰則項の効果が変化し，モデル決定に対して影響力の小さい変数は$w_i$が0に近づく．
しかし，この手法では$w_i$が0に近づくほど第二項の影響が小さくなるので$w_i$は完全には0にならず，閾値処理をしないと$w_i$を0にできない．

そこで，Lasso回帰が考案された．
Lasso回帰は罰則項にL1ノルムを用いた手法である．

\begin{eqnarray}
\label{eq:3-12}
L1norm = ||x||_1 = (|x_0|^1 + |x_1|^1 + |x_2|^1 + ...)^{\frac{1}{1}} = \sum_i |x_i|
\end{eqnarray}

L1ノルムとはつまり，係数行列の絶対値の和である．
こちらも変数の数（係数の非0の項の数）についての制約はない．
しかし，実際には係数の一部が0になる．
それを以下で説明する．

一般化したLasso回帰の最小化式は次式となる．

\begin{eqnarray}
\label{eq:3-13}
min E({\bf w}) + \lambda \sum_{i} | w_i |
\end{eqnarray}

式\ref{eq:3-13}はRidge回帰の時と違い単に偏微分の逆行列では解が出ない．
Lasso回帰を解くアルゴリズムはLARS\cite{a21}やGPS\cite{a22}，CDA\cite{a23}などが存在する．

\if0
分量が足りなければこれらのアルゴリズムの説明
\fi
式\ref{eq:3-13}を式\ref{eq:3-11}と同じように$w_i$で微分すると次式になる．

\begin{eqnarray}
\label{eq:3-14}
\frac{\partial E({\bf w})}{\partial w_i} + \lambda \cdot sgn(w_i)
\end{eqnarray}

sgn(x)は符号関数である．
これも先と同様に考えると$w_i$は第二項を入れない場合と比較し，0に近づく．
しかし，符号関数は$w_i$の正負のみで$w_i$の大小に関わらない値を持つ．
そのため第一項が小さく，モデルに対する影響の小さい変数は$w_i = 0$に収束する．
これにより，Ridge回帰のような閾値処理を用いることなく，係数が0となり変数が削減される方向に最小化式が変化していく．

Lasso回帰は影響の小さい変数の係数が0に収束するが，あくまで罰則項は係数の絶対値の和である．
そのためある変数が別の変数と従属の関係にあり，モデルに与える影響が大きい場合，どちらの変数もモデルに残ってしまう．
つまりモデル構築に不要な変数が存在しているが，Lasso回帰では削減できない場合がある．

そこで罰則項として，係数行列 $\theta$ のL0ノルムと重み$\lambda$ の積を加えた次式を考える．

\begin{eqnarray}
\label{eq:3-2}
\argmin_{\theta} \left( ||{\bf Y}-{\bf X}\theta||_{2}^{2} + \lambda \sum_i ||\theta_i ||_0 \right)
\end{eqnarray}

式\ref{eq:3-8}は$p\geq 1$の場合で定義されているが，それをp=0の場合にも拡張することでL0ノルムは次式で表せる．

\begin{eqnarray}
\label{eq:3-3}
L0norm = ||x||_0 = (|x_0|^0 + |x_1|^0 + |x_2|^0 + ...)^{\frac{1}{0}} = \sum_i \left( |x_i|^0 \right) ^ \infty
\end{eqnarray}

$0^0=0$ と定義した場合，$x_i = 0$のとき$0^{\infty} = 0$ ，$x_i \neq 0$のとき$1^{\infty} = 1$となる．
つまり，式(\ref{eq:3-3})は$x_i$ の非ゼロ成分の数となる．
この場合，罰則項は変数の数に比例するのでLasso回帰の問題のように変数同士が従属の関係にある場合であっても変数の数が削減されるようになる．
また，Lasso回帰と比較して変数の数が少なくなる傾向にあるのでよりオーバーフィッティングの問題が発生しにくい．

本研究は以上の理由より，L0ノルムを罰則項としてモデル構築に使用する．
次に，最適化式\ref{eq:3-2}をどのようにして解くか説明する．
\if0
log (x*y*z) = logx +logy + logz
logCPUE = log(Ax1*Ax2*...) = logAx1 + logAx2 + ...
この研究だと
log(sumAx)なのでこうはならないけど，上のようにしたらどうなるか
\fi